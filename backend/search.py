import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import faiss
import numpy as np
import pickle
import pandas as pd
import logging
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from backend.config import FAISS_INDEX_PATH, BM25_INDEX_PATH, DATASET_PATH, FAISS_TOP_K, BM25_WEIGHT
from backend.chat_history import add_search_results_to_history

search_router = APIRouter()

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# 1. ì¿¼ë¦¬ ë¶„ë¥˜ í•¨ìˆ˜
def classify_query_type(query: str) -> str:
    prof_keywords = ["êµìˆ˜", "êµìˆ˜ë‹˜", "ì´ êµìˆ˜", "ë‹´ë‹¹ êµìˆ˜", "ì„ ìƒë‹˜"]
    return "professor" if any(kw in query for kw in prof_keywords) else "course"

# 2. í…ìŠ¤íŠ¸ êµ¬ì„± í•¨ìˆ˜
def get_combined_text(row, query_type: str) -> str:
    if query_type == "professor":
        return f"êµìˆ˜ëª…: {row.get('êµìˆ˜ëª…', '')} ê°•ì˜ëª…: {row.get('ê°•ì˜ëª…', '')}"
    else:
        return (
            f"ê°•ì˜ëª…: {row.get('ê°•ì˜ëª…', '')} "
            f"êµìˆ˜ëª…: {row.get('êµìˆ˜ëª…', '')} "
            f"ê°•ì˜êµ¬ì„±: {row.get('ê°•ì˜êµ¬ì„±', '')} "
            f"ê°œìš”: {row.get('êµê³¼ëª©ê°œìš”', '')}"
        )

def load_dataset():
    logging.info(f"ğŸ”¹ Loading dataset from: {DATASET_PATH}")
    try:
        return pd.read_csv(DATASET_PATH, encoding="utf-8-sig")
    except Exception as e:
        logging.error(f"Dataset load failed: {e}")
        return None

def search_course_directly(query, df):
    filtered_df = df[df["ê°•ì˜ëª…"] == query]
    if filtered_df.empty:
        return None
    course_info = filtered_df.iloc[0]
    return [{
        "ê°•ì˜ëª…": course_info["ê°•ì˜ëª…"],
        "êµìˆ˜ëª…": course_info["êµìˆ˜ëª…"],
        "ì´ìˆ˜êµ¬ë¶„": course_info["ì´ìˆ˜êµ¬ë¶„"],
        "í‰ì ": course_info.get("í‰ì ", "ì •ë³´ ì—†ìŒ"),
        "ê³¼ì œ": course_info.get("ê³¼ì œ", "ì •ë³´ ì—†ìŒ"),
        "ì¶œê²°": course_info.get("ì¶œê²°", "ì •ë³´ ì—†ìŒ"),
        "ì‹œí—˜": course_info.get("ì‹œí—˜", "ì •ë³´ ì—†ìŒ"),
        "êµê³¼ëª©ê°œìš”": course_info["êµê³¼ëª©ê°œìš”"][:150]
    }]

def load_faiss_index():
    logging.info(f"ğŸ”¹ Loading FAISS index from: {FAISS_INDEX_PATH}")
    try:
        return faiss.read_index(FAISS_INDEX_PATH)
    except Exception as e:
        logging.error(f"FAISS index load failed: {e}")
        return None

def normalize_scores(scores):
    if len(scores) == 0:
        return np.zeros_like(scores)
    min_score = np.min(scores)
    max_score = np.max(scores)
    return (scores - min_score) / (max_score - min_score + 1e-8) if max_score - min_score > 1e-8 else np.ones_like(scores)

# í•µì‹¬ í•¨ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ì¿¼ë¦¬ ìœ í˜•ë³„ í…ìŠ¤íŠ¸ êµ¬ì„±
def hybrid_search(query, top_k=FAISS_TOP_K):
    logging.info(f"\n Searching for: '{query}'")
    df = load_dataset()
    if df is None:
        logging.error("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨, ê²€ìƒ‰ ì¤‘ë‹¨")
        return []

    direct_course_result = search_course_directly(query, df)
    if direct_course_result:
        logging.info("ê°•ì˜ëª…ì„ ì§ì ‘ ì…ë ¥í•˜ì—¬ CSVì—ì„œ ê²€ìƒ‰ ì™„ë£Œ!")
        add_search_results_to_history(query, direct_course_result)
        return direct_course_result

    # ì§ˆì˜ ìœ í˜• ë¶„ë¥˜
    query_type = classify_query_type(query)
    logging.info(f"ì§ˆì˜ ìœ í˜•: {query_type}")

    # ê²€ìƒ‰ í…ìŠ¤íŠ¸ êµ¬ì„± (ì§ˆì˜ ìœ í˜•ë³„)
    combined_texts = [get_combined_text(row, query_type) for _, row in df.iterrows()]
    bm25 = BM25Okapi([doc.split() for doc in combined_texts])

    # ì„ë² ë”© & FAISS ê²€ìƒ‰
    embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    query_vector = embedding_model.encode([query]).astype('float32')
    faiss_index = load_faiss_index()
    if faiss_index is None:
        return []

    faiss.normalize_L2(query_vector)
    D, I = faiss_index.search(query_vector, len(df))

    # BM25 ê²€ìƒ‰
    tokenized_query = query.split()
    bm25_scores = np.array(bm25.get_scores(tokenized_query))

    # ì ìˆ˜ ì •ê·œí™” & ê²°í•©
    faiss_scores = np.zeros(len(df))
    faiss_scores[I[0]] = D[0]
    faiss_norm = normalize_scores(faiss_scores)
    bm25_norm = normalize_scores(bm25_scores)
    combined_scores = (1 - BM25_WEIGHT) * faiss_norm + BM25_WEIGHT * bm25_norm

    # ìƒìœ„ ê²°ê³¼ ì¶”ì¶œ
    top_indices = np.argsort(combined_scores)[::-1]
    search_results = []

    for idx in top_indices:
        score = combined_scores[idx]
        if score < 0.5 or len(search_results) >= top_k: #ì ìˆ˜ í•„í„°ë§
            continue
        row = df.iloc[idx]
        search_results.append({
            "í•™ê³¼": row["í•™ê³¼"],
            "ê°•ì˜ëª…": row["ê°•ì˜ëª…"],
            "ê°œì„¤í•™ê¸°": row["ê°œì„¤í•™ê¸°"],
            "êµìˆ˜ëª…": row["êµìˆ˜ëª…"],
            "í‰ì ": row["í‰ì "],
            "ê³¼ì œ": row["ê³¼ì œ"],
            "ì¡°ëª¨ì„": row["ì¡°ëª¨ì„"],
            "ì„±ì ": row["ì„±ì "],
            "ì¶œê²°": row["ì¶œê²°"],
            "ì‹œí—˜": row["ì‹œí—˜"],
            "í•™ì •ë²ˆí˜¸": row["í•™ì •ë²ˆí˜¸"],
            "ì´ìˆ˜êµ¬ë¶„": row["ì´ìˆ˜êµ¬ë¶„"], #ì¶”ê°€
            "ê°•ì˜êµ¬ì„±": row["ê°•ì˜êµ¬ì„±"],
            "ê°•ì˜ì‹œê°„": row["ê°•ì˜ì‹œê°„"],
            "êµê³¼ëª©ê°œìš”": row["êµê³¼ëª©ê°œìš”"][:150],
            "ì ìˆ˜": round(float(score), 4)
        })

    if search_results:
        add_search_results_to_history(query, search_results)
    return search_results

# API ì—”ë“œí¬ì¸íŠ¸
class Query(BaseModel):
    query: str

@search_router.post("", include_in_schema=True)
async def search_courses(query: Query):
    try:
        user_query = query.query
        df = load_dataset()
        if df is None:
            raise HTTPException(status_code=500, detail="ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨")

        direct_course_result = search_course_directly(user_query, df)
        if direct_course_result:
            logging.info("ê°•ì˜ëª…ì„ ì§ì ‘ ì…ë ¥í•˜ì—¬ CSVì—ì„œ ê²€ìƒ‰ ì™„ë£Œ!")
            add_search_results_to_history(user_query, direct_course_result)
            return {"results": direct_course_result}

        search_results = hybrid_search(user_query)
        if not search_results:
            return {"results": [], "message": "ê´€ë ¨ ê°•ì˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        return {"results": search_results}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    query = input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    results = hybrid_search(query)

    print("\nê²€ìƒ‰ ê²°ê³¼:")
    for rank, result in enumerate(results, 1):
        print(f"\n {rank}ìœ„ - {result['ê°•ì˜ëª…']} ({result['êµìˆ˜ëª…']})")
        print(f"í•™ê³¼: {result['í•™ê³¼']} / ê°œì„¤í•™ê¸°: {result['ê°œì„¤í•™ê¸°']}")
        print(f"í‰ì : {result['í‰ì ']} / ì¶œê²°: {result['ì¶œê²°']}")
        print(f"ì‹œí—˜: {result['ì‹œí—˜']} / ê³¼ì œ: {result['ê³¼ì œ']}")
        print(f"ê°œìš”: {result['êµê³¼ëª©ê°œìš”']}...")
        print(f"ì ìˆ˜: {result['ì ìˆ˜']:.4f}")

__all__ = ["search_router"]
